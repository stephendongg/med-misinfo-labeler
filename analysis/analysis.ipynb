{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776877ea",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "330e51dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts in log: 150\n"
     ]
    }
   ],
   "source": [
    "### Setup and load data\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the moderation log\n",
    "log_df = pd.read_csv('../output/moderation_log.csv')\n",
    "\n",
    "print(f\"Total posts in log: {len(log_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d8f97e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels parsed successfully\n"
     ]
    }
   ],
   "source": [
    "### Parse labels\n",
    "\n",
    "# Parse labels from JSON strings to Python lists\n",
    "log_df['expected_labels_list'] = log_df['expected_labels'].apply(\n",
    "    lambda x: json.loads(x) if pd.notna(x) and x != '' else []\n",
    ")\n",
    "log_df['predicted_labels_list'] = log_df['predicted_labels'].apply(\n",
    "    lambda x: json.loads(x) if pd.notna(x) and x != '' else []\n",
    ")\n",
    "\n",
    "# Create string versions for grouping (sorted for consistency)\n",
    "log_df['expected_labels_str'] = log_df['expected_labels_list'].apply(\n",
    "    lambda x: json.dumps(sorted(x))\n",
    ")\n",
    "log_df['predicted_labels_str'] = log_df['predicted_labels_list'].apply(\n",
    "    lambda x: json.dumps(sorted(x))\n",
    ")\n",
    "\n",
    "# Calculate correctness by comparing label lists\n",
    "log_df['is_correct'] = log_df.apply(\n",
    "    lambda row: sorted(row['expected_labels_list']) == sorted(row['predicted_labels_list']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Labels parsed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffade7",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "940c08d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Breakdown:\n",
      "                                Labels  Real Posts  Generated Posts  Total Posts\n",
      "  [\"drug-approved\", \"supported-claim\"]           9               21           30\n",
      "[\"drug-approved\", \"unsupported-claim\"]           8               21           29\n",
      "                     [\"drug-approved\"]          12               19           31\n",
      "                   [\"drug-unapproved\"]          12               18           30\n",
      "                                    []           9               21           30\n",
      "\n",
      "Total: 150 posts (50 real, 100 generated)\n"
     ]
    }
   ],
   "source": [
    "### Dataset Breakdown \n",
    "\n",
    "# Count posts by source and label combination\n",
    "breakdown = log_df.groupby(['source', 'expected_labels_str']).size().reset_index(name='count')\n",
    "\n",
    "summary_data = []\n",
    "for label_str in sorted(breakdown['expected_labels_str'].unique()):\n",
    "    label_list = json.loads(label_str)\n",
    "    label_display = json.dumps(label_list) if label_list else \"[]\"\n",
    "    \n",
    "    real = breakdown[(breakdown['expected_labels_str'] == label_str) & \n",
    "                     (breakdown['source'] == 'real')]['count'].sum()\n",
    "    generated = breakdown[(breakdown['expected_labels_str'] == label_str) & \n",
    "                          (breakdown['source'] == 'generated')]['count'].sum()\n",
    "    total = real + generated\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Labels': label_display,\n",
    "        'Real Posts': real,\n",
    "        'Generated Posts': generated,\n",
    "        'Total Posts': total\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Dataset Breakdown:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\nTotal: {summary_df['Total Posts'].sum()} posts ({summary_df['Real Posts'].sum()} real, {summary_df['Generated Posts'].sum()} generated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b498e3",
   "metadata": {},
   "source": [
    "### Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98ddb975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 137/150 = 0.913 (91.3%)\n"
     ]
    }
   ],
   "source": [
    "### Calculate the Overall Accuracy\n",
    "\n",
    "total = len(log_df)\n",
    "correct = log_df['is_correct'].sum()\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "print(f\"Overall Accuracy: {correct}/{total} = {accuracy:.3f} ({accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "235bd180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Labels Precision Recall    F1  TP  FP  FN\n",
      "  [\"drug-approved\", \"supported-claim\"]     0.929  0.867 0.897  26   2   4\n",
      "[\"drug-approved\", \"unsupported-claim\"]     0.920  0.793 0.852  23   2   6\n",
      "                     [\"drug-approved\"]     0.912  1.000 0.954  31   3   0\n",
      "                   [\"drug-unapproved\"]     0.824  0.933 0.875  28   6   2\n",
      "                                    []     1.000  0.967 0.983  29   0   1\n"
     ]
    }
   ],
   "source": [
    "### Per-Label Combination Metrics\n",
    "\n",
    "# Calculate metrics treating each label combination as a separate class\n",
    "all_combinations = set(log_df['expected_labels_str'].unique()) | set(log_df['predicted_labels_str'].unique())\n",
    "\n",
    "results = []\n",
    "for combo_str in sorted(all_combinations):\n",
    "    combo_list = json.loads(combo_str)\n",
    "    combo_display = json.dumps(combo_list) if combo_list else \"[]\"\n",
    "    \n",
    "    tp = sum((pred_str == combo_str) and (exp_str == combo_str) \n",
    "             for pred_str, exp_str in zip(log_df['predicted_labels_str'], log_df['expected_labels_str']))\n",
    "    fp = sum((pred_str == combo_str) and (exp_str != combo_str) \n",
    "             for pred_str, exp_str in zip(log_df['predicted_labels_str'], log_df['expected_labels_str']))\n",
    "    fn = sum((pred_str != combo_str) and (exp_str == combo_str) \n",
    "             for pred_str, exp_str in zip(log_df['predicted_labels_str'], log_df['expected_labels_str']))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'Labels': combo_display,\n",
    "        'Precision': f\"{precision:.3f}\",\n",
    "        'Recall': f\"{recall:.3f}\",\n",
    "        'F1': f\"{f1:.3f}\",\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9eef4",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5782f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "Average time per post: 9.24s\n",
      "Average memory usage: 0.38 MB\n",
      "Average LLM calls: 2.0\n",
      "Average FDA calls: 1.4\n",
      "\n",
      "Totals:\n",
      "Total LLM calls: 300\n",
      "Total FDA calls: 214\n"
     ]
    }
   ],
   "source": [
    "### Performance Metrics: \n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"Average time per post: {log_df['time_seconds'].mean():.2f}s\")\n",
    "print(f\"Average memory usage: {log_df['memory_mb'].mean():.2f} MB\")\n",
    "print(f\"Average LLM calls: {log_df['llm_calls'].mean():.1f}\")\n",
    "print(f\"Average FDA calls: {log_df['fda_calls'].mean():.1f}\")\n",
    "print(f\"\\nTotals:\")\n",
    "print(f\"Total LLM calls: {log_df['llm_calls'].sum()}\")\n",
    "print(f\"Total FDA calls: {log_df['fda_calls'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d86f4",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aba90d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error dataset created with 13 errors\n",
      "Saved to: aanlysis/output/error_analysis.csv\n",
      "\n",
      "Error breakdown:\n",
      "error_type\n",
      "Wrong Labels      10\n",
      "False Negative     2\n",
      "False Positive     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "911d2fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE BY CLAIM STATUS\n",
      "================================================================================\n",
      "\n",
      "Posts WITH claims (53 posts):\n",
      "  Average time: 14.86s\n",
      "  Average LLM calls: 3.17\n",
      "  Average FDA calls: 2.26\n",
      "\n",
      "Posts WITHOUT claims (97 posts):\n",
      "  Average time: 6.17s\n",
      "  Average LLM calls: 1.36\n",
      "  Average FDA calls: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Performance for posts with vs without claims\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE BY CLAIM STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "has_claim_df = log_df[log_df['has_claim'] == True]\n",
    "no_claim_df = log_df[log_df['has_claim'] == False]\n",
    "\n",
    "print(f\"\\nPosts WITH claims ({len(has_claim_df)} posts):\")\n",
    "print(f\"  Average time: {has_claim_df['time_seconds'].mean():.2f}s\")\n",
    "print(f\"  Average LLM calls: {has_claim_df['llm_calls'].mean():.2f}\")\n",
    "print(f\"  Average FDA calls: {has_claim_df['fda_calls'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nPosts WITHOUT claims ({len(no_claim_df)} posts):\")\n",
    "print(f\"  Average time: {no_claim_df['time_seconds'].mean():.2f}s\")\n",
    "print(f\"  Average LLM calls: {no_claim_df['llm_calls'].mean():.2f}\")\n",
    "print(f\"  Average FDA calls: {no_claim_df['fda_calls'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5a8e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TIME DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Posts by processing time:\n",
      "  <5s: 34 posts (22.7%)\n",
      "  5-10s: 58 posts (38.7%)\n",
      "  10-15s: 36 posts (24.0%)\n",
      "  15-20s: 15 posts (10.0%)\n",
      "  >20s: 7 posts (4.7%)\n"
     ]
    }
   ],
   "source": [
    "# Time distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TIME DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Categorize posts by processing time\n",
    "log_df['time_category'] = pd.cut(log_df['time_seconds'], \n",
    "                                  bins=[0, 5, 10, 15, 20, float('inf')],\n",
    "                                  labels=['<5s', '5-10s', '10-15s', '15-20s', '>20s'])\n",
    "\n",
    "time_dist = log_df['time_category'].value_counts().sort_index()\n",
    "print(\"\\nPosts by processing time:\")\n",
    "for category, count in time_dist.items():\n",
    "    pct = (count / len(log_df)) * 100\n",
    "    print(f\"  {category}: {count} posts ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933640d0",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9b3eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error dataset created with 13 errors\n",
      "Saved to: analysis/outputs/error_analysis.csv\n",
      "\n",
      "Error breakdown:\n",
      "error_type\n",
      "Wrong Labels      10\n",
      "False Negative     2\n",
      "False Positive     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Error Analysis\n",
    "\n",
    "# Create error dataset\n",
    "incorrect = log_df[log_df['is_correct'] == False].copy()\n",
    "\n",
    "# Add error type classification\n",
    "error_types = []\n",
    "for idx, row in incorrect.iterrows():\n",
    "    exp_set = set(row['expected_labels_list'])\n",
    "    pred_set = set(row['predicted_labels_list'])\n",
    "    \n",
    "    fp_labels = pred_set - exp_set\n",
    "    fn_labels = exp_set - pred_set\n",
    "    \n",
    "    if len(fp_labels) > 0 and len(fn_labels) == 0:\n",
    "        error_types.append(\"False Positive\")\n",
    "    elif len(fn_labels) > 0 and len(fp_labels) == 0:\n",
    "        error_types.append(\"False Negative\")\n",
    "    else:\n",
    "        error_types.append(\"Wrong Labels\")\n",
    "\n",
    "incorrect['error_type'] = error_types\n",
    "\n",
    "# Select relevant columns for error analysis\n",
    "error_dataset = incorrect[[\n",
    "    'source', 'input', 'expected_labels', 'predicted_labels', \n",
    "    'error_type', 'drugs_detected', 'has_claim', 'claim_text',\n",
    "    'time_seconds', 'llm_calls', 'fda_calls'\n",
    "]].copy()\n",
    "\n",
    "# Save to CSV in output directory\n",
    "error_dataset.to_csv('outputs/error_analysis.csv', index=False)\n",
    "\n",
    "print(f\"Error dataset created with {len(error_dataset)} errors\")\n",
    "print(f\"Saved to: analysis/outputs/error_analysis.csv\")\n",
    "print(\"\\nError breakdown:\")\n",
    "print(error_dataset['error_type'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
